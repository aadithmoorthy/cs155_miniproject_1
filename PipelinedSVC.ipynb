{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models trained on full training dataset.  \n",
    "\n",
    "Pipeline is as follows:  \n",
    "1) Data is either raw word counts or word counts capped at one  \n",
    "2) Data is sent through PCA without whitening (linear SVC) or with whitening (RBF SVC)  \n",
    "3) Cross validation is used to chose the best L1-regularized LinearSVC  \n",
    "4) The LinearSVC defines the subspace of features used for final SVC  \n",
    "5) Cross validation is used to choose the best SVC on reduced feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_unsplit_data, get_test_data\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "X_train, y_train, = get_unsplit_data()\n",
    "X_test = get_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncate word counts to one\n",
    "X_train_trunc = np.minimum(X_train, np.ones(X_train.shape))\n",
    "X_test_trunc = np.minimum(X_test, np.ones(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA based on raw input data\n",
    "raw_PCA = PCA()\n",
    "raw_PCA.fit(X_train)\n",
    "\n",
    "# Transform training and test data\n",
    "X_train_raw_PCA = raw_PCA.transform(X_train)\n",
    "X_test_raw_PCA = raw_PCA.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whitened PCA based on raw input data\n",
    "raw_PCA_white = PCA(whiten=True)\n",
    "raw_PCA_white.fit(X_train)\n",
    "\n",
    "# Transform training and test data\n",
    "X_train_raw_PCA_white = raw_PCA_white.transform(X_train)\n",
    "X_test_raw_PCA_white = raw_PCA_white.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA based on truncated input data\n",
    "trunc_PCA = PCA()\n",
    "trunc_PCA.fit(X_train_trunc)\n",
    "\n",
    "# Transform training and test data\n",
    "X_train_trunc_PCA = trunc_PCA.transform(X_train_trunc)\n",
    "X_test_trunc_PCA = trunc_PCA.transform(X_test_trunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whitened PCA based on truncated input data\n",
    "trunc_PCA_white = PCA(whiten=True)\n",
    "trunc_PCA_white.fit(X_train_trunc)\n",
    "\n",
    "# Transform training and test data\n",
    "X_train_trunc_PCA_white = trunc_PCA_white.transform(X_train_trunc)\n",
    "X_test_trunc_PCA_white = trunc_PCA_white.transform(X_test_trunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Stage Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidate_L1_LinearSVC(X_t, y_t, C_params, K_folds):\n",
    "    \n",
    "    start = time.time()\n",
    "    # Set parameters to be crossvalidated\n",
    "    tuned_parameters = [{'loss':['squared_hinge'], 'penalty':['l1'], 'dual':[False], 'C': C_params}]\n",
    "    # Perform cross validation\n",
    "    clf = GridSearchCV(LinearSVC(), tuned_parameters, cv=K_folds, scoring='accuracy')\n",
    "    clf.fit(X_t, y_t)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"L1 Cross-validation Training Time = \", (end - start))\n",
    "    print()\n",
    "\n",
    "    print(\"Best parameter set found on development set:\")\n",
    "    print(clf.best_params_)\n",
    "    bestmodel = clf.best_estimator_\n",
    "    N_coef = np.sum(bestmodel.coef_[0] != 0)\n",
    "    N_dim = len(bestmodel.coef_[0])\n",
    "    print(\"Dimensionality of model: %s of %s\" % (N_coef, N_dim))\n",
    "    print()\n",
    "    \n",
    "    return bestmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidate_final_LinearSVC(X_t, y_t, C_params, K_folds):\n",
    "    \n",
    "    start = time.time()\n",
    "    # Set parameters to be crossvalidated\n",
    "    tuned_parameters = [{'C': C_params, 'kernel':['linear'], 'cache_size':[2000]}]\n",
    "    # Perform cross validation\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=K_folds, scoring='accuracy')\n",
    "    clf.fit(X_t, y_t)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Linear SVC Cross-validation Training Time = \", (end - start))\n",
    "    print()\n",
    "\n",
    "    print(\"Best parameter set found on development set:\")\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    \n",
    "    print(\"Retraining model for decision probabilities\")\n",
    "    best_param = clf.best_params_\n",
    "    best_param['probability'] = True\n",
    "    \n",
    "    start = time.time()\n",
    "    # Train Final Model\n",
    "    model = SVC(**best_param)\n",
    "    model.fit(X_t, y_t)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Final Linear SVC Training Time = \", (end - start))\n",
    "    print()\n",
    "    \n",
    "    nSupp = len(model.support_)\n",
    "    fCorr = np.sum(model.predict(X_t) == y_t)/len(y_t)\n",
    "    print(\"Number of Support Vectors = \", nSupp)\n",
    "    print(\"Training Accuracy = \", fCorr)\n",
    "    print()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossvalidate_final_rbfSVC(X_t, y_t, C_params, G_params, K_folds):\n",
    "    \n",
    "    start = time.time()\n",
    "    # Set parameters to be crossvalidated\n",
    "    tuned_parameters = [{'C': C_params, 'gamma': G_params, 'kernel':['rbf'], 'cache_size':[2000]}]\n",
    "    # Perform cross validation\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=K_folds, scoring='accuracy')\n",
    "    clf.fit(X_t, y_t)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Final RBF SVC Cross-validation Training Time = \", (end - start))\n",
    "    print()\n",
    "\n",
    "    print(\"Best parameter set found on development set:\")\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "\n",
    "    print(\"Retraining model for decision probabilities\")\n",
    "    best_param = clf.best_params_\n",
    "    best_param['probability'] = True\n",
    "    \n",
    "    start = time.time()\n",
    "    # Train Final Model\n",
    "    model = SVC(**best_param)\n",
    "    model.fit(X_t, y_t)\n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"Final RBF SVC Training Time = \", (end - start))\n",
    "    print()\n",
    "    \n",
    "    nSupp = len(model.support_)\n",
    "    fCorr = np.sum(model.predict(X_t) == y_t)/len(y_t)\n",
    "    print(\"Number of Support Vectors = \", nSupp)\n",
    "    print(\"Training Accuracy = \", fCorr)\n",
    "    print()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_data(final_model, X_PCA_test_reduced):\n",
    "    # Predict labels of dataset\n",
    "    y_labels = final_model.predict(X_PCA_test_reduced)\n",
    "    # Prediction probabilities\n",
    "    y_probs = final_model.predict_proba(X_PCA_test_reduced)\n",
    "    return [y_labels, y_probs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predicted_labels(y_label, file_prefix):\n",
    "    y_label = y_label.reshape((len(y_label),1))\n",
    "    result_col_1 = (np.array(range(len(y_label)))+1).reshape((len(y_label),1))\n",
    "    result = np.concatenate((result_col_1,y_label), axis = 1)\n",
    "    np.savetxt(file_prefix + \"_pred_labels.txt\", result, fmt=\"%d\", delimiter=',', header='Id,Prediction')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predicted_probs(y_probs, file_prefix):\n",
    "    result_col_1 = (np.array(range(len(y_probs[:,0])))+1).reshape((len(y_probs[:,0]),1))\n",
    "    result = np.concatenate((result_col_1,y_probs), axis = 1)\n",
    "    np.savetxt(file_prefix + \"_pred_probs.txt\", result, fmt=[\"%d\", \"%f\", \"%f\"], delimiter=',', header='Id,P[0],P[1]')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearSVC_pipeline(PCA_model, X_t, y_t, X_v, C_init, K_init, C_final, K_final, file_prefix):\n",
    "    # Save PCA model\n",
    "    pickle.dump( PCA_model, open( file_prefix + \"_pca.p\", \"wb\" ) )\n",
    "    # Train L1-regularized linear SVC for dimension reduction\n",
    "    reducing_model = crossvalidate_L1_LinearSVC(X_t, y_t, C_init, K_init)\n",
    "    # Save model\n",
    "    pickle.dump( reducing_model, open( file_prefix + \"_reducing_svc.p\", \"wb\" ) )\n",
    "    # Reduce dimension of dataset\n",
    "    X_t_reduced = X_t[:,(reducing_model.coef_ != 0)[0]]\n",
    "    X_v_reduced = X_v[:,(reducing_model.coef_ != 0)[0]]\n",
    "    # Train final linear SVC\n",
    "    final_model = crossvalidate_final_LinearSVC(X_t_reduced, y_t, C_final, K_final)\n",
    "    # Save model\n",
    "    pickle.dump( final_model, open( file_prefix + \"_final_svc.p\", \"wb\" ) )\n",
    "    # Predict test labels and probabilities\n",
    "    [y_label, y_prob] = predict_test_data(final_model, X_v_reduced)\n",
    "    # Save labels and probabilities\n",
    "    save_predicted_labels(y_label, file_prefix)\n",
    "    save_predicted_probs(y_prob, file_prefix)\n",
    "    return [reducing_model, final_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbfSVC_pipeline(PCA_model, X_t, y_t, X_v, C_init, K_init, C_final, G_final, K_final, file_prefix):\n",
    "    # Save PCA model\n",
    "    pickle.dump( PCA_model, open( file_prefix + \"_pca.p\", \"wb\" ) )\n",
    "    # Train L1-regularized linear SVC for dimension reduction\n",
    "    reducing_model = crossvalidate_L1_LinearSVC(X_t, y_t, C_init, K_init)\n",
    "    # Save model\n",
    "    pickle.dump( reducing_model, open( file_prefix + \"_reducing_svc.p\", \"wb\" ) )\n",
    "    # Reduce dimension of dataset\n",
    "    X_t_reduced = X_t[:,(reducing_model.coef_ != 0)[0]]\n",
    "    X_v_reduced = X_v[:,(reducing_model.coef_ != 0)[0]]\n",
    "    # Train final rbf SVC\n",
    "    final_model = crossvalidate_final_rbfSVC(X_t_reduced, y_t, C_final, G_final, K_final)\n",
    "    # Save model\n",
    "    pickle.dump( final_model, open( file_prefix + \"_final_svc.p\", \"wb\" ) )\n",
    "    # Predict test labels and probabilities\n",
    "    [y_label, y_prob] = predict_test_data(final_model, X_v_reduced)\n",
    "    # Save labels and probabilities\n",
    "    save_predicted_labels(y_label, file_prefix)\n",
    "    save_predicted_probs(y_prob, file_prefix)\n",
    "    return [reducing_model, final_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Linear SVC trained on PCA of raw data\n",
    "\n",
    "C_i = np.logspace(-2,2,9)\n",
    "K_i = 10\n",
    "\n",
    "C_f = np.logspace(-2,2,9)\n",
    "K_f = 5\n",
    "\n",
    "fileprefix = 'Linear_SVC_rawPCA'\n",
    "\n",
    "[rModel, fModel] = linearSVC_pipeline(raw_PCA, X_train_raw_PCA, y_train, X_test_raw_PCA, C_i, K_i, C_f, K_f, fileprefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC trained on PCA of truncated data\n",
    "\n",
    "C_i = np.logspace(-2,2,9)\n",
    "K_i = 10\n",
    "\n",
    "C_f = np.logspace(-2,2,9)\n",
    "K_f = 5\n",
    "\n",
    "fileprefix = 'Linear_SVC_truncPCA'\n",
    "\n",
    "[rModel, fModel] = linearSVC_pipeline(trunc_PCA, X_train_trunc_PCA, y_train, X_test_trunc_PCA, C_i, K_i, C_f, K_f, fileprefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF SVC trained on whitened PCA of raw data\n",
    "\n",
    "C_i = np.logspace(-2,2,9)\n",
    "K_i = 10\n",
    "\n",
    "C_f = np.logspace(-1,1,3)\n",
    "G_f = np.logspace(-4,-2,3)\n",
    "K_f = 3\n",
    "\n",
    "fileprefix = 'RBF_SVC_whitenedRawPCA_'\n",
    "\n",
    "[rModel, fModel] = rbfSVC_pipeline(raw_PCA_white, X_train_raw_PCA_white, y_train, X_test_raw_PCA_white, C_i, K_i, C_f, G_f, K_f, fileprefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RBF SVC trained on whitened PCA of truncated data\n",
    "\n",
    "C_i = np.logspace(-2,2,9)\n",
    "K_i = 10\n",
    "\n",
    "C_f = np.logspace(-1,1,3)\n",
    "G_f = np.logspace(-4,-2,3)\n",
    "K_f = 3\n",
    "\n",
    "fileprefix = 'RBF_SVC_whitenedTruncPCA_'\n",
    "\n",
    "[rModel, fModel] = rbfSVC_pipeline(trunc_PCA_white, X_train_trunc_PCA_white, y_train, X_test_trunc_PCA_white, C_i, K_i, C_f, G_f, K_f, fileprefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
